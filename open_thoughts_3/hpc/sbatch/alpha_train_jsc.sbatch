#!/bin/bash
#SBATCH --nodes={num_nodes}          
#SBATCH --cpus-per-task={cpus_per_node}      
#SBATCH --gres=gpu:{gpus_per_node}
#SBATCH --time={time_limit}
#SBATCH --mem={mem_per_node}
#SBATCH --exclusive
#SBATCH --account={account}
#SBATCH --output={experiments_dir}/logs/%x_%j.out
#SBATCH --job-name={job_name}
#SBATCH --mail-type=END,TIME_LIMIT,FAIL
#SBATCH --mail-user=dcft-slurm-notifs-aaaap7wt363mcsgryaejj2o6dm@dogs-and-ml.slack.com

# CUDA
module load release/24.04  GCCcore/12.3.0
module load CUDA/12.1.1
module load NCCL/2.18.3-CUDA-12.1.1
export PATH=/usr/local/cuda-12/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-12/lib64:$LD_LIBRARY_PATH

# DCFT
export DCFT=/data/horse/ws/ryma833h-DCFT_Shared
source $DCFT/dcft_private/hpc/dotenv/alpha.env
source $DCFT/dcft_private/database/access.env
echo "Loading conda: $DCFT_PRIVATE_ACTIVATE_ENV"
$DCFT_PRIVATE_ACTIVATE_ENV
cd $DCFT_PRIVATE
export TRITON_CACHE_DIR=$DCFT/triton_cache
export PYTHONPATH=$DCFT_PRIVATE:$PYTHONPATH


# NETWORKING
export NCCL_NET_GDR_LEVEL=PIX # Use GPU Direct RDMA when GPU and NIC are on the same PCI switch
export NCCL_SOCKET_IFNAME=ib0
export NCCL_IB_TIMEOUT=120
export NCCL_DEBUG=INFO
MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_ADDR="${MASTER_ADDR}"
export MASTER_PORT=20043


# MISC
export CUDA_VISIBLE_DEVICES="0,1,2,3,4,5,6,7"
export NUM_NODES=$SLURM_JOB_NUM_NODES
export NUM_GPUS_PER_NODE={gpus_per_node}
export NUM_GPUS=$((NUM_GPUS_PER_NODE*SLURM_NNODES))
export SRUN_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}
export OMP_NUM_THREADS=1
DEEPSPEED_CONFIG_FILE={deepspeed}
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# PATHS
CONFIG={train_config_path_out}
OUTPUT_DIR={experiments_dir}
echo -e "CONFIG: $CONFIG\nOUTPUT_DIR: $OUTPUT_DIR"
TMP_DIR=$OUTPUT_DIR/tmp
mkdir -p $OUTPUT_DIR
mkdir -p $TMP_DIR

# ACCELERATE CONFIG
ACCELERATE_CONFIG_FILE="$TMP_DIR/${SLURM_JOB_ID}_accelerate_config.yaml.autogenerated"
cat << EOT > $ACCELERATE_CONFIG_FILE
# WARNING: do not edit this file as this is an slurm-auto-generated file
compute_environment: LOCAL_MACHINE
deepspeed_config:
  deepspeed_multinode_launcher: standard
  deepspeed_config_file: $DEEPSPEED_CONFIG_FILE
  zero3_init_flag: true
distributed_type: DEEPSPEED
fsdp_config:
machine_rank: 0
main_process_ip: $MASTER_ADDR
main_process_port: $MASTER_PORT
main_training_function: main
num_machines: $SLURM_NNODES
num_processes: $NUM_GPUS
use_cpu: false
EOT

# MAIN COMMAND ARGS
LAUNCHER="python -u -m accelerate.commands.launch \
    --rdzv_conf "rdzv_backend=c10d,rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT" \
    --config_file $ACCELERATE_CONFIG_FILE \
    --machine_rank \$SLURM_PROCID \
    --role \$(hostname -s): --tee 3 \
    "

CMD="$LAUNCHER dcft/train/llamafactory/src/train.py $CONFIG"
SRUN_ARGS="
    --nodes=$NUM_NODES \
    --gres=gpu:$NUM_GPUS_PER_NODE \
    --cpus-per-task=$SLURM_CPUS_PER_TASK \
    --wait=60 \
    --kill-on-bad-exit=1 \
    --label \
    --jobid $SLURM_JOBID"

# RUN THE MAIN COMMAND
srun $SRUN_ARGS bash -c "$CMD"
