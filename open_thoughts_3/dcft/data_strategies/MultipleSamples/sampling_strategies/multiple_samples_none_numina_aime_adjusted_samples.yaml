operators:
  - id: load_numina_math
    config:
      type: "hf_source"
      dataset: "AI-MO/NuminaMath-CoT"
      split: train
  - id: select_amc_aime
    config:
      type: function
      function: data_strategies.commons.select_rows
      function_config:
        conditions:
          source: amc_aime
    input_ids:
      - load_numina_math
  - id: select_aops
    config:
      type: function
      function: data_strategies.commons.select_rows
      function_config:
        conditions:
          source: aops_forum
    input_ids:
      - load_numina_math
  - id: sample_dataset
    config:
      type: function
      function: data_strategies.commons.uniform_sample_limited
      function_config:
        num_samples: 7_500  # NOTE: 1.25% of 8000 target from stage 1
    input_ids:
      - select_amc_aime
      - select_aops
  - id: annotate
    config:
      type: completions
      map: chat
      map_config:
        user_message_column: problem
        output_column: r1_distill_70b_response
      model: "together_ai/deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
      batch: False
      temperature: 1.0
      backend_params:
        max_requests_per_minute: 250
        max_tokens_per_minute: 500_000
        invalid_finish_reasons: ['content_filter']
      generation_params:
        max_tokens: 32_000
    input_ids:
      - sample_dataset
  - id: convert_to_sharegpt
    config:
      type: function
      function: data_strategies.commons.convert_instruction_response_to_sharegpt
      function_config:
        input_instruction_column: problem
        input_response_column: r1_distill_70b_response
        output_sharegpt_column: conversations
    input_ids:
      - annotate

  